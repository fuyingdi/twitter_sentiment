import pandas
import sklearn
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import GaussianNB,MultinomialNB
import random

test_data = []
train_data = list(map(lambda kv:(int(kv[0]),kv[1]), [line.split(', ') for line in open("test_out.dat")]))
pick_index = open('__rand_pick').read().split(',')[:-1]
[test_data.append(train_data[int(i)]) for i in pick_index]
train_data = [a for i,a in enumerate(train_data) if str(i) not in pick_index]
# for i in range(100):
#     index = random.randint(0,400)
#     test_data.append(train_data[index])
#     del(train_data[index])
train_scores = list([a[0] for a in train_data])
train_tweets = list([a[1] for a in train_data])

# test_data = list(map(lambda kv:(int(kv[0]),kv[1]), [line.split(', ') for line in open("test_out.dat")]))
test_tweets = list([a[1] for a in test_data])
test_scores = list([a[0] for a in test_data])

# words = stopwords.words('english')

vectorizer = TfidfVectorizer()
x = vectorizer.fit_transform(train_tweets).toarray()
# print(x)
# print(data[0])

# word = vectorizer.get_feature_names()

clf = MultinomialNB(alpha=9.9, fit_prior=False, class_prior=None)
clf.fit(x, train_scores)
# test = vectorizer.transform(["I hate trump","I love tru"])
# print(test.toarray())
# result = clf.predict(test.toarray())

# print(result)

v = vectorizer.transform(test_tweets).toarray()
res = clf.predict_proba(v)
_res = clf.predict(v)
# print(res)

# edge = 0.5
# for i,line in enumerate(res):
#     _res.append(0)
#     if line[0]>edge:
#         _res[i] = 0
#     elif line[0]<1-edge:
#         _res[i] = 4
#     else:
#         _res[i] = 2

        
# print(_res)

miss = 0 
out_res = [0,0,0]
for i,score in enumerate(list(_res)):
    out_res[int(score/2)] += 1
    # print(str(i)+':'+str(score) + ' ' + str(test_scores[i]))
    if score!=test_scores[i]:
        miss+=1
print("@N-BAYES:miss {}/{}, hit rate:{}%".format(miss, len(list(_res)), 100-100*miss/len(list(_res))))
with open('__nby', 'w+') as file:
    [file.write(str(i)+',') for i in out_res]
    file.write(str(100-miss))

# for t in test_tweet:
#     v = vectorizer.transform([t])
#     res = clf.predict(v.toarray()[-1])



def get_feature_vector(tweet):
    pass
    uni_feature_vector = [] # 一元特征向量
    bi_feature_vector = [] # 二元特征向量
    words = tweet.split()
    for word in words:
        if unigrams.get(word):
            uni_feature_vector.append(word)
    return uni_feature_vector

    # for i in range(len(words) - 1):
    #     word = words[i]
    #     if unigrams.get(word):
    #         uni_feature_vector.append(word)
    #     # if USE_BIGRAMS:
        #     if bigrams.get((word, next_word)):
        #         bi_feature_vector.append((word, next_word)


def extract_features(tweets):
    num_batches = int(np.ceil(len(tweets) / float(batch_size)))
    for i in xrange(num_batches):
        batch = tweets[i * batch_size: (i + 1) * batch_size]
        features = lil_matrix((batch_size, VOCAB_SIZE))
        labels = np.zeros(batch_size)
        for j, tweet in enumerate(batch):
            if test_file:
                tweet_words = tweet[1][0]
                tweet_bigrams = tweet[1][1]
            else:
                tweet_words = tweet[2][0]
                tweet_bigrams = tweet[2][1]
                labels[j] = tweet[1]
            if feat_type == 'presence':
                tweet_words = set(tweet_words)
                tweet_bigrams = set(tweet_bigrams)
            for word in tweet_words:
                idx = unigrams.get(word)
                if idx:
                    features[j, idx] += 1
            if USE_BIGRAMS:
                for bigram in tweet_bigrams:
                    idx = bigrams.get(bigram)
                    if idx:
                        features[j, UNIGRAM_SIZE + idx] += 1
        yield features, labels


def apply_tf_idf(X):
    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)
    transformer.fit(X)
    return transformer


def get_tweets(csv_file, test_file=True):
    # 读取处理过的csv返回元组列表
    # [(id, 特征向量),]
    """Returns a list of tuples of type (tweet_id, feature_vector)
            or (tweet_id, sentiment, feature_vector)
    Args:
        csv_file (str): Name of processed csv file generated by preprocess.py
        test_file (bool, optional): If processing test file
    Returns:
        list: Of tuples
    """
    tweets = []
    with open(csv_file, 'r') as csv:
        lines = csv.readlines()
        total = len(lines)
        for i, line in enumerate(lines):
            if test_file:
                tweet_id, tweet = line.split(',')
            else:
                tweet_id, sentiment, tweet = line.split(',')
            feature_vector = get_feature_vector(tweet)
            if test_file:
                tweets.append((tweet_id, feature_vector))
            else:
                tweets.append((tweet_id, int(sentiment), feature_vector))
            utils.write_status(i + 1, total)
    return tweets



    '''
    i = 1
    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))
    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):
        utils.write_status(i, n_train_batches)
        i += 1
        if FEAT_TYPE == 'frequency':
            tfidf = apply_tf_idf(training_set_X)
            training_set_X = tfidf.transform(training_set_X)
        clf.partial_fit(training_set_X, training_set_y, classes=[0, 1])
    if TRAIN:
        correct, total = 0, len(val_tweets)
        i = 1
        batch_size = len(val_tweets)
        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))
        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):
            if FEAT_TYPE == 'frequency':
                val_set_X = tfidf.transform(val_set_X)
            prediction = clf.predict(val_set_X)
            correct += np.sum(prediction == val_set_y)
            utils.write_status(i, n_val_batches)
            i += 1
        print ('\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))
    else:
        del train_tweets
        test_tweets = get
    _tweets(TEST_PROCESSED_FILE, test_file=True)
        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))
        predictions = np.array([])
        print ('Predicting batches')
        i = 1
        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):
            if FEAT_TYPE == 'frequency':
                test_set_X = tfidf.transform(test_set_X)
            prediction = clf.predict(test_set_X)
            predictions = np.concatenate((predictions, prediction))
            utils.write_status(i, n_test_batches)
            i += 1
        predictions = [(str(j), int(predictions[j]))
                       for j in range(len(test_tweets))]
        utils.save_results_to_csv(predictions, 'naivebayes.csv')
        print ('\nSaved to naivebayes.csv')
        '''